{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import FinanceDataReader as fdr\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import argparse\n",
    "from copy import deepcopy \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_set(filename):\n",
    "    dir_path = './data_set'\n",
    "    \n",
    "    with open(join(dir_path, filename), 'r') as f:\n",
    "        results = json.load(f)\n",
    "        \n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = load_data_set('my_data_set.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list=[]\n",
    "\n",
    "for sym in result[0]:\n",
    "    df = get_data(sym)\n",
    "    ds_train = StockDataset(df, 5, 1, '2000-01-01', '2018-01-01')\n",
    "    ds_valid = StockDataset(df, 5, 1, '2018-01-01', '2019-01-01')\n",
    "    \n",
    "    partition = {'train': ds_train, 'val':ds_valid}\n",
    "    dataset_list.append(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sym):\n",
    "    file_path = \"./stockData/\"\n",
    "    csv_path = os.path.join(file_path, f\"{sym}.csv\")\n",
    "    df = pd.read_csv(csv_path, parse_dates=True, index_col = ['Date'])\n",
    "    df.drop('Symbol', axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, x_frames, y_frames, start, end):\n",
    "        self.x_frames = x_frames\n",
    "        self.y_frames = y_frames\n",
    "\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "        self.data = data.loc[self.start : self.end]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - (self.x_frames + self.y_frames) + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx += self.x_frames\n",
    "        data = self.data.iloc[idx-self.x_frames:idx+self.y_frames]\n",
    "        data = data[['Close','Open', 'High', 'Low', 'Volume', 'Change']]\n",
    "        data = data.values\n",
    "        X = data[:self.x_frames]\n",
    "        y = data[self.x_frames:]\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, batch_size, dropout, use_bn):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.use_bn = use_bn \n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.regressor = self.make_regressor()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "    \n",
    "    def make_regressor(self):\n",
    "        layers = []\n",
    "        if self.use_bn:\n",
    "            layers.append(nn.BatchNorm1d(self.hidden_dim))\n",
    "        layers.append(nn.Dropout(self.dropout))\n",
    "        \n",
    "        layers.append(nn.Linear(self.hidden_dim, self.hidden_dim // 2))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.hidden_dim // 2, self.output_dim))\n",
    "        regressor = nn.Sequential(*layers)\n",
    "        return regressor\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y_pred = self.regressor(lstm_out[-1].view(self.batch_size, -1))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(y_pred, y_true):\n",
    "    perc_y_pred = np.exp(y_pred.cpu().detach().numpy())\n",
    "    perc_y_true = np.exp(y_true.cpu().detach().numpy())\n",
    "    mae = mean_absolute_error(perc_y_true, perc_y_pred, multioutput='raw_values')\n",
    "    return mae*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, loss_fn, args):\n",
    "\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for i, (X, y) in enumerate(trainloader):\n",
    "\n",
    "        X = X.to(args.device)\n",
    "        y = y.to(args.device)\n",
    "\n",
    "        X = X.transpose(0, 1).float()\n",
    "        y_true = y[:, :, 0].float()\n",
    "        \n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred.view(-1), y_true.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += metric(y_pred, y_true)[0]\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    train_acc = train_acc / len(trainloader)\n",
    "    return model, train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, partition, loss_fn, args):\n",
    "    valloader = DataLoader(partition['val'], \n",
    "                           batch_size=args.batch_size, \n",
    "                           shuffle=False, drop_last=True)\n",
    "    model.eval()\n",
    "\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(valloader):\n",
    "            X = X.to(args.device)\n",
    "            y = y.to(args.device)\n",
    "\n",
    "            X = X.transpose(0, 1).float()\n",
    "            y_true = y[:, :, 0].float()\n",
    "\n",
    "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "            y_pred = model(X)\n",
    "    \n",
    "            loss = loss_fn(y_pred.view(-1), y_true.view(-1))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_acc += metric(y_pred, y_true)[0]\n",
    "\n",
    "    val_loss = val_loss / len(valloader)\n",
    "    val_acc = val_acc / len(valloader)\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(partition, args):\n",
    "    hash_key = hashlib.sha1(str(vars(args)).encode()).hexdigest()[:6]\n",
    "\n",
    "    model = LSTM(args.input_dim, args.hid_dim, args.y_frames, args.n_layers, args.batch_size, args.dropout, args.use_bn)\n",
    "    model.to(args.device)\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice. Choose one of SGD, RMSprop, Adam')\n",
    "    \n",
    "    # ===== List for epoch-wise data ====== #\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    # ===================================== #\n",
    "\n",
    "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
    "        \n",
    "        start = time.time()\n",
    "        model, train_loss, train_acc = train(model, partition, optimizer, loss_fn, args)\n",
    "        val_loss, val_acc = validate(model, partition, loss_fn, args)\n",
    "        end = time.time()\n",
    "\n",
    "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'.format(epoch+1, train_acc, val_acc, train_loss, val_loss, end-start))\n",
    "        \n",
    "        # ====== Add Epoch Data ====== #\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        # ============================ #\n",
    "\n",
    "    # ======= Add Result to Dictionary ======= #\n",
    "    result = {}\n",
    "    result['train_losses'] = [x for x in train_losses]\n",
    "    result['val_losses'] = [x for x in val_losses]\n",
    "    result['train_accs'] = [x for x in train_accs]\n",
    "    result['val_accs'] = [x for x in val_accs]\n",
    "    result['train_acc'] = train_acc\n",
    "    result['val_acc'] = val_acc\n",
    "    \n",
    "    return model, vars(args), result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "def save_exp_result(setting, result):\n",
    "    exp_name = setting['exp_name']\n",
    "\n",
    "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
    "    \n",
    "    filepath = './results'\n",
    "    if not os.path.isdir(filepath):\n",
    "        os.mkdir(filepath)\n",
    "\n",
    "    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n",
    "    result.update(setting)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(result, f)\n",
    "\n",
    "    \n",
    "def load_exp_result(exp_name):\n",
    "    dir_path = './results'\n",
    "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n",
    "    list_result = []\n",
    "    for filename in filenames:\n",
    "        if exp_name in filename:\n",
    "            with open(join(dir_path, filename), 'r') as infile:\n",
    "                results = json.load(infile)\n",
    "                list_result.append(results)\n",
    "    df = pd.DataFrame(list_result)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = \"exp1_lr\"\n",
    "args.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Data Loading ====== #\n",
    "args.symbol = 'AAPL'\n",
    "args.batch_size = 16\n",
    "args.x_frames = 5\n",
    "args.y_frames = 1\n",
    "\n",
    "df = get_data('AAPL')\n",
    "df_scaled = Min_Max_Scaler(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = StockDataset(df_scaled, args.x_frames, args.y_frames, '2000-01-01', '2018-01-01')\n",
    "valset = StockDataset(df_scaled, args.x_frames, args.y_frames, '2018-01-01', '2019-01-01')\n",
    "\n",
    "partition = {'train': trainset, 'val':valset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Model Capacity ===== #\n",
    "args.input_dim = 6\n",
    "args.hid_dim = 50\n",
    "args.n_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Regularization ======= #\n",
    "args.l2 = 0.0001\n",
    "args.dropout = 0.3\n",
    "args.use_bn = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Optimizer & Training ====== #\n",
    "args.optim = 'Adam' #'RMSprop' #SGD, RMSprop, ADAM...\n",
    "args.lr = 0.0001\n",
    "args.epoch = 100\n",
    "args.path = './model_checkpoint/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Experiment Variable ====== #\n",
    "name_var1 = 'hid_dim'\n",
    "name_var2 = 'n_layers'\n",
    "list_var1 = [10, 50, 100]\n",
    "list_var2 = [1,2,3]\n",
    "\n",
    "model_dict = {}\n",
    "\n",
    "for var1 in list_var1:\n",
    "    for var2 in list_var2:\n",
    "        setattr(args, name_var1, var1)\n",
    "        setattr(args, name_var2, var2)\n",
    "        print(args)\n",
    "                \n",
    "        model, setting, result = experiment(partition, deepcopy(args))\n",
    "        hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
    "        model_dict[hash_key] = model\n",
    "        save_exp_result(setting, result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
